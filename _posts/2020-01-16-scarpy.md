---
title:	scarpy introduction
tags:	爬虫
---

1. 创建项目

   ```
   scrapy startproject projactname
   ```

2. 创建Spider

   ```
   cd projectname
   scrapy genspider spidername domain
   ```

   spider 中有`name, allowed_domains, start_urls, parse`

   -  name：是spider name 
   - allowed_domains：允许爬取的域名
   -  start_urls：包含spider启动时爬取的url列表
   - parse：spider中一个方法，默认按照start_urls中链接完成请求后返回的response就传给了parse, parse就可以处理response

3. 创建item, item用来保存爬到的数据，使用方法类似字典

   在item.py中按照提示添加就可以，如

   ```python
   import scrapy
   
   class DoubanItem(scrapy.Item):
     group_name=scrapy.Field()
     group_url=scrapy.Field()
   ```

   item使用的时候就和正常的class一样，但是item里的定义的那些东西就要像访问字典一样访问，如

   ```python
   class DoubanSpider(scrapy.Spider):
     def parse(self, response):
       item=DoubanItem()
       item['group_id']=xxxx # response里提取的css或者什么别的东西
   ```

4. 解析response

   response可以用CSS, XPath选择题提取元素，使用CSS例子如下：

   ```python
   def parse(self, response):
     groups=response.css('.group')
     for group in groups:
       text=group.css('.text::text').extract_first()
       tags=group.css('.tags .tag::text').extract()
   ```

   class是啥就可以用`.`啥选择器选择，要获取选择的节点中的文本就可以`::text`，这个时候得到的是一个list，可以extract_first()得到第一个元素，如果你要选的不止一个元素就可以extract()

5. 解析之后的请求

   解析完当前页面后，要拉到下一页或者跳转到某一页，怎么搞呢？使用`scrapy.Request`就可以了, `scarpy.Request`有两个参数: `url, callback`

   - url: 请求链接
   - callback: 回掉函数，request完成后response返回给callback, callback进行解析或生成下一个请求，其实也就是parse

   举个例子

   ```python
   next=response.css('.group a::attr(href)').extract_first() # 有的可能还需要response.urljoin()
   yield scrapy.Request(url=next, callback=self.parse)
   # 这里还可以用 request.follow(href,callback=self.parse)
   yield response.follow(url,callback=self.parse) 
   ```

   获得a超链接中href方法固定`::attr(href)`

   `follow()`支持相对地址，不需要`response.urljoin(href)`拼接

6. 运行spider

   ```
   scrapy crawl douban
   ```

7. 爬到的数据存储

   ```
   scrapy crawl douban -o xxx.json/.jl/.jsonlines/.csv/.xml/.pickle/.marshal/ftp://user:pass@ftp.xxx/path/to/xxxx.csv
   ```

   最后一个是输出到ftp中，要配置正确的用户名、密码、地址、输出路径

   但是这样还不够，如果我们要将数据存到数据库中怎么办呢？可以用item pipeline

8. `start_requests()`必须返回一个可迭代`scrapy.Request()`,也就是必须`yield scrapy.Request(url=url,callback=self.parse)`

   这里的start_request其实就和`class Spider`里的`start_urls=[]`功能一样

9. Scrapy shell 尝试从选择器里提取数据

   ```
   scrapy shell url
   // scrapy shell 'http://www.douban.com/'
   ```

10. css提取器

    ```python
    response.css('title::text').getall() # 返回结果包括标签，是一个完整的元素
    response.css('title::text').get() # 返回第一个结果
    response.css('titel::text').re(r'xxxxx') # re()可以写正则表达式筛选
    ```

11. 提取器还可以这样写：
    
    ```python
    response.css("div.quote")
    response.css("span.text")
    ```
    其实就是网页源代码里的蓝字标签部分.class（dbq不是很懂html表述可能不标准）

12. `spider`类中除了实现默认的提取器`parse()`之外，还可以自定义接下来提取的函数，如`def parse_author(self,response)`之类的


13. 修改`User-Agent`,`Cookies`等：
    
    1. 通过settings里的`USER_AGENT`变量

    2. 通过Downloader Middleware中`process_request()`

