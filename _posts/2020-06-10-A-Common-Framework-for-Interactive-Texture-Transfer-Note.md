---
title:	论文阅读: A Common Framework for Interactive Texture Transfer 
tags:	CV CFITT 论文阅读
---

> 读怡芳师姐的2018年CVPR论文的笔记
>
> 我太菜了，本文大概会先翻译一下论文理解大概意思然后讲讲代码的样子...?

[原文地址](https://menyifang.github.io/projects/CFITT/CFITT.html)(👈🏿这里有paper和code)

*先声明博客里的reference part不是原文里标的re，是个人贴的一些助于理解的网站，因此参考也用圆括号表示...*

## title: 一个通用的交互式纹理迁移框架

### Abstract

本文提出了一个交互式的纹理迁移问题的通用解决方案，更好地保持了局部结构和视觉丰富性。(不太懂这个*visual richness*)由于任务的多样性和需要的用户指导的简单性，这个任务很有挑战性。我们的通用框架核心思想是使用多个自定义渠道来动态指导综合过程。对于交互性，用户可以通过语义通道控制风格化纹理的空间分布。通过两个阶段的结构信息自动提取和传播获得的结构指导，为初始化提供了先验，并通过搜索具有一定结构一致性的最邻域NFF来保持显著结构。与此同时，纹理的连续性被发现同样保持和源图像相似的风格。此外我们利用具有扩展的NNF和矩阵运算改进的PatchMatch(1,2)去高速获得迁移的有更丰富的几何信息的source patches(可转换源patch?)通过与最新算法的比较，我们证明了我们方法在各种场景下的的有效性和优越性。

### 1. Introduction

介绍了纹理迁移的前人工作，包括涂鸦迁移、字体风格迁移balabala。然而由于特定的使用场景，这些已有方法看起来彼此孤立。实际上他们有一个纹理迁移的共同概念 -- 在用户的指导下，即用户应该能根据需要从source迁移纹理到target的任何地方。

本文的主要目的就是建立一个为了多任务的，用户指导下的纹理迁移的通用框架，包括将涂鸦变成艺术品，编辑装饰图案，生成有特殊效果的文本和控制文本图像中的效果分布及交换纹理。

由于任务的多样性和用户指导的简单性，使用现有方法实现上述目标很有挑战性。[33, 43]中的一些方法表现很好但是他们是针对特定领域量身定制的。Hertzmann等[21]提出了一个更通用的称为*Image Analogy*(图像类比)的解决方法。然而由于缺少足够的结构分布指导，它遭受了内部纹理错位，和保留局部高频结构失败。通过特征绘画[36]允许用户利用直线和轮廓去指导纹理迁移。它通过使用画笔工具和填充工具处理直线特征和面特征，提出了一种改进。然而，该方法更适合填充几乎固定的纹理，因为它不提供用于内部纹理生成的方向控制。[8]中提供的神经涂鸦使用卷积神经网络不能重现有低级纹理细节的清晰和高质量的图像。最近提出的Deep Image Analogy（深度图像类比）[31]产生了令人信服的结果，通过一个图像类比和神经网络的组合。然而当我们将一个涂鸦图像送到网络中时，因为语义标签的神经激活程度非常低，因此很难在无纹理的区域中建立对应关系，因此无法生成令人满意的合成结果。

本文中我们提出了一个用户指导下的通用纹理迁移框架，有能力处理各种有挑战的任务。基于交互式结构的图像综合生成受semantic map(语义图)和structure information(结构信息)的指导。语义通道被用户注释，用户可以控制目标图像中的风格化纹理的空间分布。结构化通道然后可以被内容感知的显著性检测自动提取到，并从源样式图像中传播到目标对象，作为一个先验。具体来说，传播步骤通过在源和目标图像之间注册关键轮廓点，获得内部结构对应关系。结合语义和结构信息进行动态指导，使迁移过程可以产生有内容感知和低等细节的高质量纹理。此外，用扩展的最近邻和矩阵运算改进的PatchMatch算法被采用，可以在不降低速度情况下提供更丰富的源patch。本文的主要贡献如下：

- 我们设计了一个

###  Reference

1. [Wikipedia: PatchMatch](https://en.wikipedia.org/wiki/PatchMatch)

2. [PatchMatch分析, CSDN](https://blog.csdn.net/z6491679/article/details/50807689)