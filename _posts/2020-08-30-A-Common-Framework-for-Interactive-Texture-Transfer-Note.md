---
title:	论文阅读：A Common Framework for Interactive Texture Transfer 
tags:	CV 论文阅读 SATT
---

> 在阅读了一些文章（发现我的阅读方法有些问题效率也有些问题Orz）之后，本博决定开设一个[论文阅读]([http://xusy2333.cn/tag/#/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB](http://xusy2333.cn/tag/#/论文阅读))的专栏，专栏主要记录文章思路、流程、核心公示和算法的推导之类的，并简单阐述自己的看法。大概就是这样...?
>
> 今天重写了一下读师姐的CFTT的论文的笔记


<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

[原文地址](https://menyifang.github.io/projects/CFITT/CFITT.html)(👈🏿这里有paper和code)

*先声明博客里的reference part不是原文里标的re，是个人贴的一些助于理解的网站，因此参考也用圆括号表示...*

## title: 一个通用的交互式纹理迁移框架

### Abstract

本文提出了一个保留了local structure, visual richness的一个通用交互式纹理迁移框架。用户通过输入semantic map控制生成，其实就是通过输入语义图里含有的多个标签控制纹理的分布，使用用户指定的多个自定义渠道来动态指导合成过程。

具体的structure guided通过两个阶段完成：结构信息的自动提取和结构信息的传播

通过两个阶段的结构信息自动提取和传播获得的结构指导，为初始化提供了先验，并通过搜索具有一定结构一致性的最邻域NFF来保持显著结构。与此同时，纹理的连续性被发现同样保持和源图像相似的风格。此外我们利用具有扩展的NNF和矩阵运算改进的PatchMatch(1,2)去高速获得迁移的有更丰富的几何信息的source patches(可转换源patch?)通过与最新算法的比较，我们证明了我们方法在各种场景下的的有效性和优越性。

### 1. Introduction

介绍了纹理迁移的前人工作，包括涂鸦迁移、字体风格迁移balabala。然而由于特定的使用场景，这些已有方法看起来彼此孤立。实际上他们有一个纹理迁移的共同概念 -- 在用户的指导下，即用户应该能根据需要从source迁移纹理到target的任何地方。

本文的主要目的就是建立一个为了多任务的，用户指导下的纹理迁移的通用框架，包括将涂鸦变成艺术品，编辑装饰图案，生成有特殊效果的文本和控制文本图像中的效果分布及交换纹理。

由于任务的多样性和用户指导的简单性，使用现有方法实现上述目标很有挑战性。[33, 43]中的一些方法表现很好但是他们是针对特定领域量身定制的。Hertzmann等[21]提出了一个更通用的称为*Image Analogy*(图像类比)的解决方法。然而由于缺少足够的结构分布指导，它遭受了内部纹理错位，和保留局部高频结构失败。通过特征绘画[36]允许用户利用直线和轮廓去指导纹理迁移。它通过使用画笔工具和填充工具处理直线特征和面特征，提出了一种改进。然而，该方法更适合填充几乎固定的纹理，因为它不提供用于内部纹理生成的定向的控制。[8]中提供的神经涂鸦使用卷积神经网络不能重现有低级纹理细节的清晰和高质量的图像。最近提出的Deep Image Analogy（深度图像类比）[31]产生了令人信服的结果，通过一个图像类比和神经网络的组合。然而当我们将一个涂鸦图像送到网络中时，因为语义标签的神经激活程度非常低，因此很难在无纹理的区域中建立对应关系，因此无法生成令人满意的合成结果。

本文中我们提出了一个用户指导下的通用纹理迁移框架，有能力处理各种有挑战的任务。基于交互式结构的图像生成受semantic map(语义图)和structure information(结构信息)的指导。语义通道被用户注释，用户可以控制目标图像中的风格化纹理的空间分布。结构化通道然后可以被内容感知的显著性检测自动提取到，并从源样式图像中传播到目标对象，作为一个先验。具体来说，传播步骤通过在源和目标图像之间注册关键轮廓点，获得内部结构对应关系。结合语义和结构信息进行动态指导，使迁移过程可以产生有内容感知和低等细节的高质量纹理。此外，用扩展的最近邻和矩阵运算改进的PatchMatch算法被采用，可以在不降低速度情况下提供更丰富的源patch。本文的主要贡献如下：

- 我们设计了一个通用的框架去处理交互式纹理迁移问题，并面临任务多样性和简化导图的挑战，并且展示了我们框架在多任务中的有效性。

- 我们提出了一个提取显著结构区域并且传递原图像中的结构信息到目标图像中的方法。结构信息然后被用作先导，去指导更好的合成过程。

- 我们提出了一些新颖的用户控制的纹理迁移方案，其中通过结合改进的纹理生成方法，可以更快地生成更精细的详细合成图像。

### 2. Related Work

纹理迁移可以用传统方法处理，也可以用神经网络处理，这里简单介绍几个典型方法。

#### 2.1 Classic Texture Transfer: 传统纹理迁移

传统纹理迁移方法是一个具有给定纹理示例的纹理合成的变体。早期的大部分算法都是基于Efros和Freeman提出的基于示例的纹理合成方法。他们利用对应图和相应的量如强度，约束合成过。Criminisi等人的后续工作利用patch优先级进行区域优先级以保留结构。Komodakis等人采用置信传播作为优化方案，以避免贪心patch分配。基于优化的纹理迁移技术最早由Kwatra等提出，他们也是基于示例的后续工作。由于它的高视觉质量结果和不同场景的广泛应用，这个技术发展成为一个成功的纹理合成方法。Kwatra等人将纹理合成技术看做全局优化任务，并用了类似EM(期望最大化)的算法迭代最小化能量函数。Wexler等人通过多级别合成缓解了completion issue避免陷入局部极小值。Barnes等人引入PatchMatch算法，利用随机搜索和图像中的自然的连贯性，加速最近邻搜索过程。基于优化的方法已扩展到图像融合、风格化3D渲染，和使用自适应patch分区的文本效果迁移。

然而这些方法不能合成有显著结构的纹理，并且容易因过度使用低频纹理而导致清除效果。我们方法和这些技术有相同的baseline，并且克服了使用多通道动态指引的挑战。

基于类比的方法是纹理迁移的另一种可选方案。最初在[21]中提出的Image Analogy（图像类比）利用输入样例对（source image A, stylized result A')的可用性去获得target image B的风格化图像B'。这个方法找到了source到target的每个pixel的最佳对应关系。Cheng等通过半监督学习和image quilting model(或许是“图像缝”模型)改进了这个方法，旨在确保局部和全局的连续性。这个方法也已扩展为解决动画风格化问题，并为大型数据集构建了有效查询。不幸的是，它并不提供定向控制，容易导致内部纹理错位，进而导致结构信息丢失。

#### 2.2 Neural-based Style Transfer: 基于神经的风格迁移（style译成风格会好点？

Gatys等人提出了一个利用预训练的深度卷积网络，如VGG-19的神经风格迁移方法。他们的方法对于给定风格的图像，风格化context image（上下文图像？）非常有效，由于能分解和重组图像的内容和样式。Johnson等人随后利用perceptual loss functions（感知损失函数？）训练前馈网络进行实时的纹理迁移任务。Li和Wand将马尔可夫随机场模型与深度神经网络结合起来，后来这个模型被扩展到语义风格迁移。尽管基于神经网络的方法取得了巨大的成功，但是它并不适合于我们的场景，我们的场景中源图像不局限在艺术作品，还包括照片和写实图像。对于这类数据，基于神经网络的方法通常包含许多低级噪声。此外，没有提供直观的方法去控制合成过程，因此结果变的不可预测。

最近提出的Generative Adversarial Networks(GANs)生成对抗网络提供了一个潜在的选择方案，通过对抗过程生成纹理。GAN训练一个鉴别器去区分输出是真的还是假的，同时训练一个生成器以欺骗鉴别器。最近，使用基于U-Net架构生成generator和使用卷积PatchGAN分类器生成discriminator进行鉴别的image-to-image图像到图像的转换被提出。他是一个将输入图像转换为相应输出图像的通用框架，如将语义标签、边缘、线段转换为逼真的图像。尽管这个技术产生了令人深刻的结果，但它需要收集成千上万个相关图像去训练一个特定类别的模型。相反，我们的方法仅需一个示例就可从一个对应的语义图生成目标风格化图像。

### 3. Method Description 方法描述

交互式纹理迁移旨在在用户的指导下，从给定的源图片生成风格化的目标图像。用户可以通过语义图semantic maps，控制目标图像中要生成对象的形状、规模和空间分布。通过三个输入图像$S_{sem}$(源图像的语义图)、$S_{sty}$(与$S_{sem}$对应的风格化源图像)、$T_{sem}$(目标图像的语义图)，风格化目标图像的$T_{sty}$可以被自动生成，大概是这样的过程：$S_{sem}:S_{sty}::T_{sem}:T_{sty}$图2描述了这个过程（交互式纹理迁移问题概述。使用三个输入图像$S_{sem},S_{sty},T_{sem}$可以生成具有$S_{sty}$风格的风格化目标图像$T_{sty}$

![figure2](/paper/CFITT/figure1.png)

通过使用含有少量信息的语义图重现一个有风格化纹理的结构图像是一个艰巨的任务。在我们的方法中，我们搜索

#### 3.1 Internal Salient Structure Extraction 内部显著性结构提取

#### 3.2 Structure Propagation 结构传播

#### 3.3 Guided Texture Transfer 

### 4. Implementation Details

### 5. Experimental Results

### 6. Conclusionß

###  Reference

1. [Wikipedia: PatchMatch](https://en.wikipedia.org/wiki/PatchMatch)

2. [PatchMatch分析, CSDN](https://blog.csdn.net/z6491679/article/details/50807689)